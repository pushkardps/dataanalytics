---
title: "week12_clustering"
author: "Pushkar"
date: "February 13, 2019"
output: html_document
---
##K-means Cluster Analysis
K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.
```{r, message = FALSE, warning=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(gridExtra) # clustering algorithms & visualization
```
We'll use the built-in R data set USArrests, which contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. It includes also the percent of the population living in urban areas

###Data Preparation
```{r}
df <- USArrests
head(df) #observe first six rows

#remove missing values
df <- na.omit(df)

#we need to scale the dataset before clustering
df <- scale(df)

head(df) #observe first six rows
```

###Clustering Distance Measures
Obervations are clustered into different groups based on how close(similar) or how far(dis-similar) are they from each other.The choice of distance measures is a critical step in clustering.It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters.The classical methods for distance measures are Euclidean and Manhattan distances.



Lets calculate distance
```{r}
#get_dist: computes distance matrix, default is the Euclidean method
distance <- get_dist(df) 
#fviz_dist: for visualizing a distance matrix
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```

###Computing k-means clustering in R
We can compute k-means in R with the kmeans function. Here will group the data into two clusters (centers = 2). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 25 will generate 25 initial configurations.
```{r}
k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
```
<ul>cluster: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.</ul>
<ul>centers: A matrix of cluster centers.</ul>
<ul>totss: The total sum of squares.</ul>
<ul>withinss: Vector of within-cluster sum of squares, one component per cluster.</ul>
<ul>tot.withinss: Total within-cluster sum of squares, i.e. sum(withinss).</ul>
<ul>betweenss: The between-cluster sum of squares.</ul>
<ul>size: The number of points in each cluster.</ul>

The groupings resulted in 2 clusters- size 20 and 30 across the four variables (Murder, Assault, UrbanPop, Rape).

```{r}
k2
```
```{r}
#visulation of the clusters
fviz_cluster(k2, data = df)

```

 <br /> One obvious problem with this method is that we get 2 clusters because we set 2 in our code.
Hence it is advisable to try out multiple clusters before we settle for an answer.
```{r}
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```
 <br /> This grouping is helpful, it tells us in which groups do different observations fall but we still do not
know the ideal number of clusters.

###Determining Optimal Clusters
There are three popular methods - Elbow method, Silhoutte method, Gap method.
```{r}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(df, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```

```{r}
#Process to compute the "Elbow method" has been wrapped up in a single function (fviz_nbclust):
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
```
 <br /> The results suggest that 4 is the optimal number of clusters.

```{r}
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
  km.res <- kmeans(df, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(df))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```
 <br /> According to silhouttem the optimal clusters number is 2.

```{r}
# compute gap statistic
set.seed(123)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

```
```{r}
fviz_gap_stat(gap_stat)

```
 <br /> The optimal number of clusters by gap method is 4. So most of the approaches suggest 4.
```{r}
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
```
```{r}
fviz_cluster(final, data = df)

```
 <br /> We can add the clusters to our initial data to do some descriptive statistics at the cluster level:
```{r}
USArrests %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

